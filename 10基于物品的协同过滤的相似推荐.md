# 原理

## 整体思想

- 如果两个商品有同样的受众（感兴趣的人群），那么它们就是有内在相关性的。所以可以利用已有的行为数据，分析商品受众的相似程度，进而得出商品间的相似度。我们把这种方法定义为物品的“同现相似度”

## 计算公式

“同现相似度”—利用行为数据计算不同商品间的相似度![image-20220129120652012](10%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%8E%A8%E8%8D%90.assets/image-20220129120652012.png)

- 其中，Ni是购买商品i（或对商品i评分）的用户列表，Nj是购买商品j的用户列表 
- 分子表示，同时购买了商品i和j的用户
- 分母表示的是惩罚项。如果同时购买了商品i和j的人很少，那么分母就会很大。这就说明，两个商品同时出现的概率小，那么在这个商品i的界面中推荐商品j的概率就减少
- 分子表示同时购买商品i和j的人数，分母表示购买i和购买j的人数的乘积开根号

# 创建项目

- 在commender项目下创建子项目，并命名为：iemCFRecommender
- 添加如下依赖

```pom
  <dependencies>
        <!-- Spark的依赖引入 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
        </dependency>
        <!-- 引入Scala -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
        </dependency>

        <!-- 加入MongoDB的驱动 -->
        <!-- 用于代码方式连接MongoDB -->
        <dependency>
            <groupId>org.mongodb</groupId>
            <artifactId>casbah-core_2.11</artifactId>
            <version>${casbah.version}</version>
        </dependency>
        <!-- 用于Spark和MongoDB的对接 -->
        <dependency>
            <groupId>org.mongodb.spark</groupId>
            <artifactId>mongo-spark-connector_2.11</artifactId>
            <version>${mongodb-spark.version}</version>
        </dependency>
    </dependencies>
```

- 在resources文件夹下添加log4j文件
- 将java文件夹改名为scala
- 在scala文件夹下创建scala class 文件，object类型，命名为：ItemCFRecommender

# 定义样例类

- 需要定义MongoDB连接配置样例类
- 需要拿到用户评分数据，所以需要用户评分样例类
- 最后需要产生推荐列表，所以需要有产品推荐样例类

```scala
  case class ProductRating(userId: Int, productId: Int, score: Double, timestamp: Int);

  case class MongoConfig(uri: String, db: String);

  case class Recommendation(productId: Int, score: Double);

  case class ProductRecs(productId: Int, recs: Seq[Recommendation]);
```

# 定义常量和表名

- 评分表
- 写入数据库的表
- 限制每个商品产生的推荐商品数量

```scala
    // 定义常量和表名
    val MONGODB_RATING_COLLECTION = "Rating"
    val ITEM_CF_PRODUCT_RECS = "ItemCFProductRecs"
    val MAX_RECOMMENDER_NUMS = 10
```

# 连接配置，spark session 以及隐式参数定义

- MongoDB连接配置项
- 创建spark session
- 创建隐式变量

```scala
      val config = Map(
        // 启用本地多线程
        "spark.cores" -> "local[*]",
        // 定义Mongo的uri
        "mongo.uri" -> "mongodb://localhost:27017/recommender",
        // 定义db
        "mongo.db" -> "recommender"
      )
      // 创建spark配置项
      val  sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender")
      // 创建spark session
      val spark = SparkSession.builder().config(sparkConf).getOrCreate()
      import spark.implicits._
      // 创建隐式参数
      implicit val mongoConfig = MongoConfig(config("mongo.uri"),config("mongo.db"))
```

# 加载评分数据

```scala
      val ratingDF = spark.read
        .option("uri", mongoConfig.uri)
        .option("collection", MONGODB_RATING_COLLECTION)
        .format("com.mongodb.spark.sql")
        .load()
        .as[ProductRating]
        .map(
          x => (x.userId, x.productId, x.score)
        )
        .toDF("userId", "productId", "score")
        .cache()
```

# 计算同现相似度，计算相似列表

获取临时表，用于sql查询

- 获取数据列表，格式为【userId, productId1, count1, productId2, count 2】
- count1是product1的被评分次数
- count2是product2的被评分次数

```scala
      // 统计每个商品的评分个数，按照productId来group by
      val productRatingCountDF = ratingDF.groupBy("productId").count()
      // 在原有评分表的基础上，添加count列
      val ratingWithCountDF = ratingDF.join(productRatingCountDF,"productId")
      // 将商品按照userId两两配对
      val joinedDF = ratingWithCountDF.join(ratingWithCountDF,"userId")
        // 根据产生的数据，来定义列名
        .toDF("userId", "product1", "score1", "count1", "product2", "score2", "count2")
        // 筛选出需要的列
        .select("userId", "product1", "count1", "product2", "count2")
      // 创建一张临时表，用于sql查询
      joinedDF.createOrReplaceTempView("joined")
```

## 统计出对两个商品同时做出评分的人数

- 按照product1和product2进行分组，统计出对两个商品同时做出评分的人数

```scala
      // 按照product1和product2进行分组，统计出对两个商品同时做出评分的人数
      val cooccurrenceDF = spark.sql(
        """
          |select product1, product2, count(userId) as cocount1, first(count1) as count1, first(count2) as count2
          |from joined
          |group by product1, product2
          |""".stripMargin
      )
```

## 得到simDF

```scala
      // 提取需要的数据，包装成(productId1, (product2, score))
      val simDF = cooccurrenceDF.map {
        row =>
          val cooSim = cooccurrenceSim(row.getAs[Long]("cocount"), row.getAs[Long]("count1"), row.getAs[Long]("count2"))
          // 返回第一个数是row.getInt(0)得到的是product1，row.getInt(1)得到的是product2
          (row.getInt(0), (row.getInt(1), cooSim))
      }
        .rdd
        .groupByKey()
        .map{
          case (productId, recs) =>
            ProductRecs(productId, recs.toList.filter(x => x._1 != productId)
                                              .sortWith(_._2>_._2)
                                              .take(MAX_RECOMMENDER_NUMS)
                                              .map(x => Recommendation(x._1 , x._2)))
        }
        .toDF()
```



## 定义函数cooccurrenceSim

```scala
    def cooccurrenceSim(coCount: Long, count1: Int, count2: Int): Double = {
      coCount / math.sqrt(count1*count2)
    }
```



# 整体代码

```scala
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
   * 评分样例类
   * @param userId
   * @param productId
   * @param score
   * @param timestamp
   */
  case class ProductRating(userId: Int, productId: Int, score: Double, timestamp: Int);

  /**
   * MongoDB连接配置样例类
   * @param uri
   * @param db
   */
  case class MongoConfig(uri: String, db: String);

  /**
   * 标准推荐样例类
   * @param productId
   * @param score
   */
  case class Recommendation(productId: Int, score: Double);

  /**
   * 商品相似度列表
   * @param productId
   * @param recs
   */
  case class ProductRecs(productId: Int, recs: Seq[Recommendation]);
  object ItemCFRecommender {
    // 定义常量和表名
    val MONGODB_RATING_COLLECTION = "Rating"
    val ITEM_CF_PRODUCT_RECS = "ItemCFProductRecs"
    val MAX_RECOMMENDER_NUMS = 10

    def main(args: Array[String]): Unit = {
      /**
       * 定义连接配置项、spark session 以及 隐式参数
       */
      val config = Map(
        // 启用本地多线程
        "spark.cores" -> "local[*]",
        // 定义Mongo的uri
        "mongo.uri" -> "mongodb://localhost:27017/recommender",
        // 定义db
        "mongo.db" -> "recommender"
      )
      // 创建spark配置项
      val  sparkConf = new SparkConf().setMaster(config("spark.cores")).setAppName("OfflineRecommender")
      // 创建spark session
      val spark = SparkSession.builder().config(sparkConf).getOrCreate()
      import spark.implicits._
      // 创建隐式参数
      implicit val mongoConfig = MongoConfig(config("mongo.uri"),config("mongo.db"))

      /**
       * 加载评分数据，将数据转换成DF类型
       */
      val ratingDF = spark.read
        .option("uri", mongoConfig.uri)
        .option("collection", MONGODB_RATING_COLLECTION)
        .format("com.mongodb.spark.sql")
        .load()
        .as[ProductRating]
        .map(
          x => (x.userId, x.productId, x.score)
        )
        .toDF("userId", "productId", "score")
        .cache()

      // TODO: 核心算法，计算同现相似度，计算相似列表
      // 统计每个商品的评分个数，按照productId来group by
      val productRatingCountDF = ratingDF.groupBy("productId").count()
      // 在原有评分表的基础上，添加count列
      val ratingWithCountDF = ratingDF.join(productRatingCountDF,"productId")
      // 将商品按照userId两两配对
      val joinedDF = ratingWithCountDF.join(ratingWithCountDF,"userId")
        // 根据产生的数据，来定义列名
        .toDF("userId", "product1", "score1", "count1", "product2", "score2", "count2")
        // 筛选出需要的列
        .select("userId", "product1", "count1", "product2", "count2")
      // 创建一张临时表，用于sql查询
      joinedDF.createOrReplaceTempView("joined")

      // 按照product1和product2进行分组，统计出对两个商品同时做出评分的人数
      val cooccurrenceDF = spark.sql(
        """
          |select product1, product2, count(userId) as cocount, first(count1) as count1, first(count2) as count2
          |from joined
          |group by product1, product2
          |""".stripMargin
      )
      // 提取需要的数据，包装成(productId1, (product2, score))
      val simDF = cooccurrenceDF.map {
        row =>
          val cooSim = cooccurrenceSim(row.getAs[Long]("cocount"), row.getAs[Long]("count1"), row.getAs[Long]("count2"))
          // 返回第一个数是row.getInt(0)得到的是product1，row.getInt(1)得到的是product2
          (row.getInt(0), (row.getInt(1), cooSim))
      }
        .rdd
        .groupByKey()
        .map{
          case (productId, recs) =>
            ProductRecs(productId, recs.toList.filter(x => x._1 != productId)
                                              .sortWith(_._2>_._2)
                                              .take(MAX_RECOMMENDER_NUMS)
                                              .map(x => Recommendation(x._1 , x._2)))
        }
        .toDF()

      /**
       * 保存到MongoDB
       */
      simDF.write
        .option("uri",mongoConfig.uri)
        .option("collection",ITEM_CF_PRODUCT_RECS)
        .mode("overwrite")
        .format("com.mongodb.spark.sql")
        .save()

      // 结束
      spark.stop()
    }

    def cooccurrenceSim(coCount: Long, count1: Long, count2: Long): Double = {
      coCount / math.sqrt(count1*count2)
    }

  }
```

# 测试

- 运行代码后，打开MongoDB数据库，可以发现里边多了一个数据库，：ItemCFProductRecs![image-20220222134724011](10%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%8E%A8%E8%8D%90.assets/image-20220222134724011.png)
- 查询数据![image-20220222134828237](10%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%8E%A8%E8%8D%90.assets/image-20220222134828237.png)