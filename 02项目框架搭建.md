# 创建项目

1. 创建maven项目并命名为ECommenderSystem
2. 为项目ECommenderSystem创建子项目，并命名为Commender
3. 为项目Commender创建子项目，并命名为DataLoad。
   - 注意这里创建它的子项目需要专门指定一下，默认父项目还是ECommenderSystem
4. 删除项目ECommenderSystem和Commender的src文件夹

# 修改pom文件

## 修改ECommenderSystempom的pom文件

- 在该项目的pom文件夹的properties配置版本信息

  ```pom
  <!--处理日志-->
  <log4j.version>1.2.17</log4j.version>
  <!--简单日志接口-->
  <slf4j.version>1.7.22</slf4j.version>
  <!--MongoDB和spark的连接器-->
  <mongodb-spark.version>2.0.0</mongodb-spark.version>
  <!--MongoDB的Scala的driver-->
  <casbah.version>3.1.1</casbah.version>
  <!--redies-->
  <redis.version>2.9.0</redis.version>
  <!--kafka-->
  <kafka.version>0.10.2.1</kafka.version>
  <!--spark-->
  <spark.version>2.1.1</spark.version>
  <!--Scala-->
  <scala.version>2.11.8</scala.version>
  <!--java线性代数库，做矩阵运算-->
  <jblas.version>1.2.1</jblas.version>
  ```

- 然后添加依赖，添加的是所有的子项目用到的依赖。这里放置的是所有模块都用到的依赖。在[mavenrepository](https://mvnrepository.com/search?q=slf4j)网址里可以找到这些依赖内容

  ```pom
  <dependencies>
              <!-- 引入共同的日志管理工具 -->
          <dependency>
              <groupId>org.slf4j</groupId>
              <artifactId>jcl-over-slf4j</artifactId>
              <version>1.7.35</version>
          </dependency>
          <dependency>
              <groupId>org.slf4j</groupId>
              <artifactId>slf4j-api</artifactId>
              <version>1.7.35</version>
          </dependency>
          <dependency>
              <groupId>org.slf4j</groupId>
              <artifactId>slf4j-log4j12</artifactId>
              <version>1.7.35</version>
              <type>pom</type>
              <scope>test</scope>
          </dependency>
          <dependency>
              <groupId>log4j</groupId>
              <artifactId>log4j</artifactId>
              <version>${log4j.version}</version>
          </dependency>
  </dependencies>
  ```

- 引入插件配置

  ```pom
  <build>
      <!--声明并引入子项目共有的插件-->
      <plugins>
          <plugin>
              <groupId>org.apache.maven.plugins</groupId>
              <artifactId>maven-compiler-plugin</artifactId>
              <version>3.6.1</version>
              <!--所有的编译用JDK1.8-->
              <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
              </configuration>
          </plugin>
      </plugins>
      <pluginManagement>
          <plugins>
              <!--maven的打包插件-->
              <plugin>
                  <groupId>org.apache.maven.plugins</groupId>
                  <artifactId>maven-assembly-plugin</artifactId>
                  <version>3.0.0</version>
                  <executions>
                      <execution>
                          <id>make-assembly</id>
                          <phase>package</phase>
                          <goals>
                              <goal>single</goal>
                          </goals>
                      </execution>
                  </executions>
              </plugin>
              <!--该插件用于将scala代码编译成class文件-->
              <plugin>
                  <groupId>net.alchim31.maven</groupId>
                  <artifactId>scala-maven-plugin</artifactId>
                  <version>3.2.2</version>
                  <executions>
                      <!--绑定到maven的编译阶段-->
                      <execution>
                          <goals>
                              <goal>compile</goal>
                              <goal>testCompile</goal>
                          </goals>
                      </execution>
                  </executions>
              </plugin>
          </plugins>
      </pluginManagement>
  </build>
  ```

  

## 修改Commender的pom文件

- 声明spark组件依赖

  - dependencyManagement的作用是只声明不引入

  ```pom
  <dependencyManagement>
      <dependencies>
          <!-- 引入Spark相关的Jar包 -->
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-core_2.11</artifactId>
              <version>${spark.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-sql_2.11</artifactId>
              <version>${spark.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-streaming_2.11</artifactId>
              <version>${spark.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-mllib_2.11</artifactId>
              <version>${spark.version}</version>
          </dependency>
          <dependency>
              <groupId>org.apache.spark</groupId>
              <artifactId>spark-graphx_2.11</artifactId>
              <version>${spark.version}</version>
          </dependency>
  <dependency>
              <groupId>org.scala-lang</groupId>
  <artifactId>scala-library</artifactId>
  <version>${scala.version}</version>
  </dependency>
      </dependencies>
  </dependencyManagement>
  ```

- 因为在ECommenderSystem的pom中只声明了插件，因此在Commender的pom中要引入插件

  ```pom
  <build>
      <plugins>
          <!-- 父项目已声明该plugin，子项目在引入的时候，不用声明版本和已经声明的配置 -->
          <plugin>
              <groupId>net.alchim31.maven</groupId>
              <artifactId>scala-maven-plugin</artifactId>
          </plugin>
      </plugins>
  </build>
  ```

  

## 修改DataLoad的pom文件

- 需要用到spark的一些重要依赖，因此要加上这些依赖

  ```pom
  <dependencies>
      <!-- Spark的依赖引入 -->
      <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-core_2.11</artifactId>
      </dependency>
      <dependency>
          <groupId>org.apache.spark</groupId>
          <artifactId>spark-sql_2.11</artifactId>
      </dependency>
      <!-- 引入Scala -->
      <dependency>
          <groupId>org.scala-lang</groupId>
          <artifactId>scala-library</artifactId>
      </dependency>
      <!-- 加入MongoDB的驱动 -->
      <dependency>
          <groupId>org.mongodb</groupId>
          <artifactId>casbah-core_2.11</artifactId>
          <version>${casbah.version}</version>
      </dependency>
      <dependency>
          <groupId>org.mongodb.spark</groupId>
          <artifactId>mongo-spark-connector_2.11</artifactId>
          <version>${mongodb-spark.version}</version>
      </dependency>
  </dependencies>
  ```


# DataLad项目resources文件夹

## 上传数据文件

- 将资料文件目录下的products.csv文件和ratings.csv文件放到dataload的resources文件夹下

## 设置日志输出

- 然后在该目录下创建日志文件log4j.properties

  - 代码如下

    ```properties
    # 设置数据输出级别：info，设置输出方式：stdout
    log4j.rootLogger=info, stdout
    # 设置输出方式的具体输出位置：输出到控制台
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    # 选择输出布局
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    # 设置输出格式
    log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS}  %5p --- [%50t]  %-80c(line:%5L)  :  %m%n
    ```